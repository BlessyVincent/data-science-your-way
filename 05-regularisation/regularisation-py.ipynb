{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Regularised linear models for regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Gaussian) Linear models for regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The goal of regression is to predict the value of one or more continuous *target* variables $y$ given the value of a $p$-dimensional vector $\\vec{x}$ of input variables [1]. \n",
    "\n",
    "$$\n",
    "x_1, x_2, \\dots, x_p \\rightarrow y\n",
    "$$\n",
    "\n",
    "The simplest linear regression models share the property of being linear functions of the input parameters. If we call that function $f$, a model prediction $\\hat{y}$ for a given value of the input vector $\\vec{x}$ is given by \n",
    "\n",
    "$$\n",
    "\\hat{y} = f(x_1, x_2, \\dots, x_p)\n",
    "$$\n",
    "\n",
    "The process of learning the function $f$ is what machine learning or statistical learning is about. We use a dataset D, made of $N$ labeled samples $(x_1, x_2, \\dots, x_p, y)$, to estimate the function $f$. We will get a bit deeper into this subject in the following sections, in the context of cost functions and regularisation. But in principle, for each data point, we have that \n",
    "\n",
    "\n",
    "$$\n",
    "y_i = f(\\vec{x}_i) + \\epsilon\n",
    "$$\n",
    "\n",
    "This is equivalent to say that for each sample, the target variable is given by a deterministic function (i.e. $f$) plus noise. Probably the most popular the most popular family of models comes from the assumption that the error $\\epsilon$ is normally distributed (i.e. Gaussian), which is not a bad assumption in most real world scenarios thanks to the central limit theorem. That is equivalent to say that the input vector and output variable together make a multi-variate normal distribution $(x_1, x_2, \\dots, x_p, y)$. If we take a probabilistic approach, we can describe each data point as:\n",
    "\n",
    "$$\n",
    "p(y_i | \\vec{x}_i) \\sim N(\\mu_i, \\sigma^2)\n",
    "$$\n",
    "\n",
    "that is, the target variable $y$ is normally distributed conditional to the values of $\\vec{x}$ where \n",
    "\n",
    "$$\n",
    "\\mu_i = \\beta_1 \\Phi_1(x_1) + \\dots + \\beta_r \\Phi_r(x_i)\n",
    "$$\n",
    "\n",
    "that is, we define the function $f$ (and the center of the distribution) in terms of a series of $r$ weights or coefficients $\\vec{\\beta}$ and selection functions $\\vec{\\Phi}$ that combine the values of the inputs to get the output. Finally, if we define for the whole dataset \n",
    "\n",
    "$$\n",
    "Y = \\begin{pmatrix}\n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\dots \\\\ \n",
    "y_N\n",
    "\\end{pmatrix}\n",
    "\n",
    "\\vec{\\beta} = \\begin{pmatrix}\n",
    "\\beta_1 \\\\ \n",
    "\\beta_2 \\\\ \n",
    "\\dots \\\\ \n",
    "\\beta_r\n",
    "\\end{pmatrix}\n",
    "\n",
    "X = \\begin{pmatrix}\n",
    "\\Phi_1(x_1) && \\Phi_2(x_1) && \\dots && \\Phi_r(x_1) \\\\ \n",
    "\\Phi_1(x_2) && \\Phi_2(x_2) && \\dots && \\Phi_r(x_2) \\\\ \n",
    "\\Phi_1(x_3) && \\Phi_2(x_3) && \\dots && \\Phi_r(x_3) \\\\ \n",
    "\\dots && \\dots && \\dots && \\dots \\\\\n",
    "\\Phi_1(x_N) && \\Phi_2(x_N) && \\dots && \\Phi_r(x_N) \\\\ \n",
    "\\end{pmatrix}\n",
    "$$ and \n",
    "\n",
    "we have that \n",
    "\n",
    "$$\n",
    "Y = X \\vec{\\beta} + \\epsilon\n",
    "$$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$$\n",
    "p(Y) \\sim N(X \\vec{\\beta}, \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "The problem is then to estimate the coefficients vector $\\vec{\\beta}$. Two ways of estimate these values are *least squares estimation* and *maximum likelihood estimation*. Although the are equivalent for linear gaussian models, they do not have to be. We will focus on the first approach for the time being."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Least Squares Estimation\n",
    "\n",
    "The LSE method uses a cost function $S$, the residual sum of squares, which we aim to minimise:\n",
    "\n",
    "$$\n",
    "S(\\vec{\\beta}) = \\sum_{i=1}^{N} (Y_i - \\sum_{k=1}^{r} \\beta_k \\Phi_k(x_i))^2 = (Y - X\\vec{\\beta})^T(Y-X \\vec{\\beta})\n",
    "$$\n",
    "\n",
    "We sum, over all $N$ samples in out dataset, the squared difference between the actual value $Y_i$ and the prediction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## A regularised cost function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "One of the most important steps in the machine learning model is feature selection. During that step, we decide which elements of the input vector $\\vec{x}$ are important and will be used to predict the target variable $y$. Not all elements of $\\vec{x}$ are equally (or at all) important to predict the target variable. One principled and more continuous way of doing that is by penalising the cost function $C$ in a way that it keeps only the most important elements of $\\vec{x}$. This process is called *regularisation* or *shrinkage*.\n",
    "\n",
    "Remember that our task is to find the vector of weights that minimises the resodual sum of squares:\n",
    "\n",
    "$$\n",
    "\\hat{\\vec{\\beta}} = \\underset{\\vec{\\beta}}{argmin} (Y - X \\vec{\\beta}})^T(Y-X\\vec{\\beta}})\n",
    "$$\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Regularised linear models in Python"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## References\n",
    "\n",
    "1. Bishop. *Pattern recognition and machine learning*. 2006"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}